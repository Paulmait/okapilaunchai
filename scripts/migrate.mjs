/**
 * Run Supabase migrations using direct PostgreSQL connection
 */

import pg from "pg";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { config } from "dotenv";

const __dirname = path.dirname(fileURLToPath(import.meta.url));

// Load .env file from project root
config({ path: path.join(__dirname, "..", ".env") });

// IMPORTANT: Set DATABASE_URL in your environment or .env file
// Format: postgresql://postgres:PASSWORD@db.YOUR_PROJECT.supabase.co:5432/postgres
const DATABASE_URL = process.env.DATABASE_URL;

if (!DATABASE_URL) {
  console.error("âŒ Missing DATABASE_URL environment variable");
  console.error("   Set it in your .env file or environment");
  console.error("   Format: postgresql://postgres:PASSWORD@db.PROJECT.supabase.co:5432/postgres");
  process.exit(1);
}

const migrations = [
  "0001_init.sql",
  "0002_storage_exports_bucket.sql",
  "0003_rls_policies.sql",
  "0004_analytics_and_feedback.sql",
  "20260107_subscriptions.sql",
  "0005_admin_users.sql"
];

async function runMigrations() {
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
  console.log("  OkapiLaunch AI - Running Migrations");
  console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

  const client = new pg.Client({
    connectionString: DATABASE_URL,
    ssl: { rejectUnauthorized: false }
  });

  try {
    console.log("ðŸ“¡ Connecting to database...");
    await client.connect();
    console.log("âœ… Connected!\n");

    for (const migrationFile of migrations) {
      const filePath = path.join(__dirname, "..", "supabase", "migrations", migrationFile);

      if (!fs.existsSync(filePath)) {
        console.log(`âš ï¸  Migration file not found: ${migrationFile}`);
        continue;
      }

      console.log(`ðŸ“„ Running: ${migrationFile}`);
      const sql = fs.readFileSync(filePath, "utf-8");

      try {
        await client.query(sql);
        console.log(`   âœ… Success\n`);
      } catch (err) {
        // Some errors are expected (e.g., "already exists")
        if (err.message.includes("already exists") || err.message.includes("duplicate")) {
          console.log(`   âš ï¸  Already applied (skipped)\n`);
        } else {
          console.log(`   âŒ Error: ${err.message}\n`);
        }
      }
    }

    // Verify tables exist
    console.log("ðŸ“‹ Verifying tables...");
    const tables = ["projects", "jobs", "ai_decisions", "ai_runs", "analytics_events", "user_feedback", "nps_responses", "subscriptions", "usage", "admin_users", "admin_audit_log"];

    for (const table of tables) {
      try {
        const result = await client.query(`SELECT COUNT(*) FROM ${table}`);
        console.log(`   âœ… ${table}: OK`);
      } catch (err) {
        console.log(`   âŒ ${table}: ${err.message}`);
      }
    }

    // Check storage bucket
    console.log("\nðŸ“¦ Checking storage bucket...");
    try {
      const result = await client.query(`SELECT id FROM storage.buckets WHERE id = 'exports'`);
      if (result.rows.length > 0) {
        console.log("   âœ… exports bucket: OK");
      } else {
        console.log("   âš ï¸  exports bucket: not found (will be created on first upload)");
      }
    } catch (err) {
      console.log(`   âš ï¸  Could not check bucket: ${err.message}`);
    }

    console.log("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    console.log("  âœ… Migrations complete!");
    console.log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

  } catch (err) {
    console.error("âŒ Database error:", err.message);
    process.exit(1);
  } finally {
    await client.end();
  }
}

runMigrations().catch((err) => {
  console.error("Migration failed:", err);
  process.exit(1);
});
